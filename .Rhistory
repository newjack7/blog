lincsBigrams <- lincsBigrams %>% separate(bigram, c("word1", "word2"), sep = " ")
#Filtered for stop words
lincsBigrams <- lincsBigrams %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word1 %in% lincs_stop_words$word) %>%
filter(!word2 %in% lincs_stop_words$word)
#Remove all digits
lincsBigrams <- lincsBigrams %>% filter(grepl('^\\D', word1))
lincsBigrams <- lincsBigrams %>% filter(grepl('^\\D', word2))
#New counts based on frequency
lincsBigrams %>% count(word1, word2, sort = TRUE)
#Finally I reunite the two columns for purposes of later analyses
bigramUnited <- lincsBigrams %>% unite(bigram, word1, word2, sep = " ")
bigramUnited
bigramCount <- bigramUnited %>%
group_by(bigram)  %>%
count(bigram, sort = TRUE) %>%
summarize(total = sum(n))
bigramCount
ggplot(bigramCount, aes(total)) + geom_histogram(fill="#ffd700", binwidth = 0.75, bins = 100) + theme_minimal()
#Create tri-grams
lincsTrigrams <- lincsEvents %>% unnest_tokens(trigram, QUOTE_TRANSCRIPTION, token = "ngrams", n = 3)
# Split the text into three columns based on the spaces between words
lincsTrigrams <- lincsTrigrams %>% separate(trigram, c("word1", "word2", "word3"), sep = " ")
#Filtered for stop words
lincsTrigrams <- lincsTrigrams %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(!word3 %in% stop_words$word) %>%
filter(!word1 %in% lincs_stop_words$word) %>%
filter(!word2 %in% lincs_stop_words$word) %>%
filter(!word3 %in% lincs_stop_words$word)
#Remove all digits
lincsTrigrams <- lincsTrigrams %>% filter(grepl('^\\D', word1))
lincsTrigrams <- lincsTrigrams %>% filter(grepl('^\\D', word2))
lincsTrigrams <- lincsTrigrams %>% filter(grepl('^\\D', word3))
lincsTrigrams %>% count(word1, word2, word3, sort = TRUE)
#Finally I reunite the two columns for purposes of later analyses
Trigram_united <- lincsTrigrams %>% unite(Trigram, word1, word2, word3, sep = " ")
lincsTrigrams %>% filter(word1 == "roger", word2 == "henry", word3 == "thomas")
library(tidyverse)
library(tidytext)
setwd("C:/Users/Jack/Documents/GitHub/bi-grams")
load(".Rdata")
lincs_words <- Lincolnshire_Tokens %>% count(Lincolnshire_ID, word, sort = TRUE)
total_words <- lincs_words %>% group_by(Lincolnshire_ID) %>%  summarize(total = sum(n))
lincs_words <- left_join(lincs_words, total_words)
lincs_words
freq_by_rank <- lincs_words %>%  group_by(Lincolnshire_ID) %>% mutate(rank = row_number(), 'term frequency' = n/total) %>%  ungroup()
freq_by_rank
lincs_tf_idf <- lincs_words %>% bind_tf_idf(word, Lincolnshire_ID, n)
lincs_tf_idf
lincs_tf_idf %>%
select(-total) %>%
filter(n>10) %>%
arrange(desc(tf_idf))
#Removes entries containing personal names, place names, digits, and common english words.
stop_tf_idf <- lincs_tf_idf %>% anti_join(lincs_stop_people)
stop_tf_idf <- stop_tf_idf %>% anti_join(lincs_stop_places)
stop_tf_idf <- stop_tf_idf %>% filter(grepl('^\\D', word))
stop_tf_idf <- stop_tf_idf %>% anti_join(stop_words)
stop_tf_idf %>%
filter(n>3) %>%
arrange(desc(tf))
lincsYearWords <- full_join(lincsTotalTidy, fullDated, by = 'UUID')
lincsYearWords <- lincsYearWords %>% select(UUID, lincsID, word, day, month, year, date)
lincsYearWords <- lincsYearWords %>% count(year, word, sort = TRUE)
lincsTotalYearWords <- lincsYearWords %>% group_by(year) %>%  summarize(total = sum(n))
lincsTotalYearWords <- left_join(lincsYearWords, lincsTotalYearWords)
#Remove all rows without a year
lincsTotalYearWords <- lincsTotalYearWords %>%  na.omit()
lincsTotalYearWords
yearFreqRank <- lincsTotalYearWords %>%  group_by(year) %>% mutate(rank = row_number(), 'term frequency' = n/total) %>%  ungroup()
yearFreqRank
lincsYearTFIDF <- lincsTotalYearWords %>% bind_tf_idf(word, year, n)
lincsYearTFIDF %>%
filter(n>8) %>%
arrange(desc(tf_idf))
#Number of occurrences of each distinct word once names and sums of money are removed
wordCounts <- lincsTotalTidy %>% count(word, sort = TRUE)
wordCounts <- wordCounts %>% rename('allegation' = 'word')
lincsYearTFIDF <- lincsYearTFIDF %>% rename('allegation' = word)
TFIDFCounts <- left_join(lincsYearTFIDF, wordCounts)
TFIDFCounts %>%
filter(totalDistinct>20) %>%
filter(!(allegation %in% monthOrder)) %>%
arrange(desc(tf_idf))
View(TFIDFCounts)
blogdown:::preview_site()
blogdown:::preview_site()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
#import from csv
burghershRegisterVol3 <- read_csv("Lincolnshire csv Data/burghersh register vol 3.csv")
#extract strings which are a digit followed immediately but a full stop.
burghershVol3Filter <- burghershRegisterVol3 %>% mutate(ID1 = (str_extract(ID, "[\\d]+\\.")))
#filter out the NAs
burghershVol3Filter <- burghershVol3Filter %>% filter(!is.na(ID1))
#Select the correct columns
burghershVol3Filter <- burghershVol3Filter %>% select(ID1, text)
#remove the last character from column ID1 (full stops)
burghershVol3Filter$ID1 = substr(burghershVol3Filter$ID1,1,nchar(burghershVol3Filter$ID1)-1)
#check to see if any entries are missing or duplicated
setdiff(1:5007, burghershVol3Filter$ID1)
#Turn ID1 column into a sortable numeric column
burghershVol3Filter <- transform(burghershVol3Filter, ID1 = as.numeric(ID1))
#Load the packages for spacy parsing
library(lubridate)
library(spacyr)
library(tif)
library(reticulate)
spacy_initialize()
#Create the TIF (Text Interchange Format) data structure which spacy needs as an input
burghVol3TIF <- data.frame(doc_id = burghershVol3Filter$ID1, text = burghershVol3Filter$text)
#Check if the format is correct
tif_is_corpus_df(burghVol3TIF)
#This always seems to say FALSE but it still works in spacy?
#Parse the text through Spacy
burghVol3parsed <- spacy_parse(burghVol3TIF)
slice(burghVol3parsed, 500:510)
View(burghVol3parsed)
burghConsolidate <- burghVol3parsed %>% entity_consolidate()
View(burghConsolidate)
vol3Places <- burghConsolidate %>% filter(entity == "GPE")
vol3Places <- burghConsolidate %>% filter(entity_type == "GPE")
View(vol3Places)
#count locations
placesCount <- group_by(entity, token) %>% count(token, sort = TRUE)
View(vol3Places)
#count locations
vol3PlacesCount <- group_by(entity_type, token) %>% count(token, sort = TRUE)
#count locations
vol3PlacesCount <-vol3Places  %>% count(token, sort = TRUE)
View(vol3PlacesCount)
pip install -U spacy-nightly --pre
library(reticulate)
pip install -U spacy-nightly --pre
system("which python")
spacy_initialized(python_executable ="C:\Users\Jack\AppData\Roaming\Microsoft\Windows\Start Menu\Programs\Python 3.9")
spacy_initialized(python_executable ="C:/Users/Jack/AppData/Roaming/Microsoft/Windows/Start Menu/Programs/Python 3.9")
library(reticulate)
spacy_initialized(python_executable ="C:/Users/Jack/AppData/Roaming/Microsoft/Windows/Start Menu/Programs/Python 3.9")
library(spacyr)
spacy_initialized(python_executable ="C:/Users/Jack/AppData/Roaming/Microsoft/Windows/Start Menu/Programs/Python 3.9")
spacy_initialize(python_executable ="C:/Users/Jack/AppData/Roaming/Microsoft/Windows/Start Menu/Programs/Python 3.9")
spacy_initialize(python_executable ="C:\Users\Jack\AppData\Local\Programs\Python\Python39\python.exe")
spacy_initialize(python_executable ="C:/Users/Jack/AppData/Local/Programs/Python/Python39/python.exe")
remove.packages("spacyr", lib="~/R/win-library/4.0")
devtools::install_github("quanteda/spacyr", build_vignettes = FALSE)
devtools::install_github("quanteda/spacyr", build_vignettes = FALSE)
remove.packages("RcppArmadillo", lib="~/R/win-library/4.0")
remove.packages("RcppEigen", lib="~/R/win-library/4.0")
devtools::install_github("quanteda/spacyr", build_vignettes = FALSE)
install.packages("Rcpp")
devtools::install_github("quanteda/spacyr", build_vignettes = FALSE)
install.packages("spacyr")
install.packages("spacyr")
install.packages("rtools")
devtools::install_github("quanteda/spacyr", build_vignettes = FALSE)
install.packages("spacyr")
install.packages("spacyr")
devtools::install_github("quanteda/spacyr", build_vignettes = FALSE)
spacy_initialize(python_executable ="C:/Users/Jack/AppData/Local/Programs/Python/Python39/python.exe", save_profile = TRUE)
library(reticulate)
library(spacyr)
spacy_initialize(python_executable ="C:/Users/Jack/AppData/Local/Programs/Python/Python39/python.exe", save_profile = TRUE)
spacy_initialize(python_executable ="C:/Users/Jack/AppData/Local/Programs/Python/Python39/python.exe", save_profile = TRUE)
spacy_initialize()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
#Load the packages for spacy parsing
library(lubridate)
library(tif)
library(reticulate)
library(spacyr)
spacy_initialize(model = "en_core_web_trf", save_profile = TRUE)
burghVol3TIF <- data.frame(doc_id = burghershVol3Filter$ID1, text = burghershVol3Filter$text)
#Check if the format is correct
tif_is_corpus_df(burghVol3TIF)
#Parse the text through Spacy
burghVol3parsed <- spacy_parse(burghVol3TIF)
#Parse the text through Spacy
burghVol3parsed <- spacy_parse(burghVol3TIF, lemma = FALSE, entity = TRUE, nounphrase = TRUE)
burghVol3parsed <- spacy_extract_entity(burghVol3TIF)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
#Load the packages for spacy parsing
library(lubridate)
library(tif)
library(reticulate)
library(spacyr)
spacy_initialize(model = "en_core_web_lrg", save_profile = TRUE)
spacy_initialize(model = "en_core_web_sm")
#Create the TIF (Text Interchange Format) data structure which spacy needs as an input
burghVol3TIF <- data.frame(doc_id = burghershVol3Filter$ID1, text = burghershVol3Filter$text)
#Check if the format is correct
tif_is_corpus_df(burghVol3TIF)
#This always seems to say FALSE but it still works in spacy?
#Parse the text through Spacy
#burghVol3parsed <- spacy_parse(burghVol3TIF, lemma = FALSE, entity = TRUE, nounphrase = TRUE)
burghVol3parsed <- spacy_extract_entity(burghVol3TIF)
slice(burghVol3parsed, 500:510)
#Filter for locations
vol3Places <- burghConsolidate %>% filter(entity_type == "GPE")
#count locations
vol3PlacesCount <-vol3Places  %>% count(token, sort = TRUE)
View(vol3PlacesCount)
View(burghVol3parsed)
#Filter for locations
vol3Places <- burghConsolidate %>% filter(entity_type == "GPE" | entity_type = "ORG")
#Filter for locations
vol3Places <- burghConsolidate %>% filter(entity_type == "GPE" | entity_type == "ORG")
#count locations
vol3PlacesCount <-vol3Places  %>% count(token, sort = TRUE)
write.csv(vol3PlacesCount, file = "vol3Geo.csv")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
#filter out some mistakes
vol3PlacesCount<- vol3PlacesCount %>% mutate(country = "United Kingdom") %>% rename(city = token)
View(vol3PlacesCount)
vol3Geo <- vol3PlacesCount %>% geocode(city = city, country = country, method = 'osm')
library(tidygeocoder)
vol3Geo <- vol3PlacesCount %>% geocode(city = city, country = country, method = 'osm')
View(vol3Geo)
write.csv(vol3PlacesCount, file = "vol3Geo.csv")
write.csv(vol3Geo, file = "vol3Geo.csv")
write.csv(vol3Geo, file = "vol3Geo.csv")
#import from csv
burghershRegisterVol3 <- read_csv("Lincolnshire csv Data/burghersh register vol 3.csv")
#extract strings which are a digit followed immediately but a full stop.
burghershVol3Filter <- burghershRegisterVol3 %>% mutate(ID1 = (str_extract(ID, "[\\d]+\\.")))
View(burghershVol3Filter)
#filter out the NAs
burghershVol3Filter <- burghershVol3Filter %>% filter(!is.na(ID1))
#Select the correct columns
burghershVol3Filter <- burghershVol3Filter %>% select(ID1, text)
#remove the last character from column ID1 (full stops)
burghershVol3Filter$ID1 = substr(burghershVol3Filter$ID1,1,nchar(burghershVol3Filter$ID1)-1)
#check to see if any entries are missing or duplicated
setdiff(1:5007, burghershVol3Filter$ID1)
#check to see if any entries are missing or duplicated (should return 0)
setdiff(1:5007, burghershVol3Filter$ID1)
#Turn ID1 column into a sortable numeric column
burghershVol3Filter <- transform(burghershVol3Filter, ID1 = as.numeric(ID1))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(tidytext)
#Load the packages for spacy parsing
library(lubridate)
library(tidygeocoder)
library(tif)
library(reticulate)
library(spacyr)
burghVol3TIF <- data.frame(doc_id = burghershVol3Filter$ID1, text = burghershVol3Filter$text)
#Check if the format is correct
tif_is_corpus_df(burghVol3TIF)
burghVol3parsed <- spacy_extract_entity(burghVol3TIF)
View(burghVol3parsed)
#Would prefer the large but it takes too long
spacy_initialize(model = "en_core_web_lrg")
#Filter for locations
vol3Places <- burghVol3parsed %>% filter(entity_type == "GPE" | entity_type == "ORG")
View(burghVol3parsed)
#Filter for locations - includes organisations as I want to capture 'university of oxford' as well
vol3Places <- burghVol3parsed %>% filter(ent_type == "GPE" | entity_type == "ORG")
#Filter for locations - includes organisations as I want to capture 'university of oxford' as well
vol3Places <- burghVol3parsed %>% filter(ent_type == "GPE" | ent_type == "ORG")
#count locations
vol3PlacesCount <-vol3Places  %>% count(token, sort = TRUE)
View(vol3Places)
#count locations
vol3PlacesCount <-vol3Places  %>% count(text, sort = TRUE)
View(vol3PlacesCount)
spacy_finalize()
#Would prefer the large but it takes too long
spacy_initialize(model = "en_core_web_lrg")
#Would prefer the large but it takes too long
spacy_initialize(model = "en_core_web_lg")
#Would prefer the large but it takes too long
spacy_initialize(model = "en_core_web_trf")
#This has to use the small spacy model as it takes too long otherwise
burghVol3parsed <- spacy_extract_entity(burghVol3TIF)
slice(burghVol3parsed, 500:510)
View(burghVol3parsed)
#Filter for locations - includes organisations as I want to capture 'university of oxford' as well
vol3Places <- burghVol3parsed %>% filter(ent_type == "GPE" | ent_type == "ORG")
View(vol3Places)
#count locations
vol3PlacesCount <-vol3Places  %>% count(text, sort = TRUE)
View(vol3PlacesCount)
#filter out some mistakes
vol3PlacesCount<- vol3PlacesCount %>% mutate(country = "United Kingdom") %>% rename(city = token)
#filter out some mistakes
vol3PlacesCount<- vol3PlacesCount %>% mutate(country = "United Kingdom") %>% rename(city = text)
vol3Geo <- vol3PlacesCount %>% geocode(city = city, country = country, method = 'osm')
View(vol3Geo)
View(vol3PlacesCount)
write.csv(vol3Geo, file = "vol3Geo.csv")
write.csv(vol3Geo, file = "vol3Geo.csv")
#Parse the text through Spacy
parsedTxt <- spacy_parse(txt)
txt <- data.frame(doc_id = lincsEvents$UUID, text = lincsEvents$word)
View(lincsEvents)
#setwd("C:/Users/Jack/Documents/GitHub/blog")
#load(".Rdata")
library(tidyverse)
library(lubridate)
library(tif)
library(reticulate)
txt <- data.frame(doc_id = lincsEvents$UUID, text = lincsEvents$word)
txt <- data.frame(doc_id = lincsEvents$UUID, text = lincsEvents$word)
#Create the TIF (Text Interchange Format) data structure which spacy needs as an input
lincsEvents <- select(lincsEvents, UUID, word) %>% as.numeric(lincsEvents$lincsID)
View(lincsEvents)
View(lincsEvents)
#Convert lincsID to numeric column
lincsEvents[, c(3)] <- sapply(lincsEvents[, c(3)], as.numeric)
#rename column to word for future analysis
lincsEvents <- lincsEvents %>% rename(word = QUOTE_TRANSCRIPTION)
#Delete the first digit from the allegation column. This removes McLane's index.
lincsEvents <- lincsEvents %>% mutate(word = (str_replace(word, '^\\d+', "")))
#Create the TIF (Text Interchange Format) data structure which spacy needs as an input
lincsEvents <- select(lincsEvents, UUID, word) %>% as.numeric(lincsEvents$lincsID)
#Create the TIF (Text Interchange Format) data structure which spacy needs as an input
lincsEvents <- select(lincsEvents, UUID, word)
txt <- data.frame(doc_id = lincsEvents$UUID, text = lincsEvents$word)
#Check if the format is correct
tif_is_corpus_df(txt)
#Parse the text through Spacy
parsedTxt <- spacy_parse(txt)
detailTag <- spacy_parse(txt, tag = TRUE)
```{r}
detailTag <- spacy_parse(txt, tag = TRUE)
verbDetails <- detailTag %>% filter(pos == 'VERB')
lincsIDS <- lincsEvents %>% select(lincsID, UUID, word)
View(lincsEvents)
View(verbDetails)
#Create lincsID to act as index to original print book
lincsEvents <- lincsEvents %>% mutate(lincsID = (str_extract(QUOTE_TRANSCRIPTION, '^\\w+')))
#rename column to word for future analysis
lincsEvents <- lincsEvents %>% rename(word = QUOTE_TRANSCRIPTION)
#Delete the first digit from the allegation column. This removes McLane's index.
lincsEvents <- lincsEvents %>% mutate(word = (str_replace(word, '^\\d+', "")))
#Convert lincsID to numeric column
lincsEvents[, c(3)] <- sapply(lincsEvents[, c(3)], as.numeric)
#Create lincsID to act as index to original print book
lincsEvents <- lincsEvents %>% mutate(lincsID = (str_extract(QUOTE_TRANSCRIPTION, '^\\w+')))
View(lincsEvents)
lincsEvents <- filter(lincsOCR, TYPE == 'EVENT')
lincsEvents <- lincsEvents %>%  select(QUOTE_TRANSCRIPTION, UUID)
#Create lincsID to act as index to original print book
lincsEvents <- lincsEvents %>% mutate(lincsID = (str_extract(QUOTE_TRANSCRIPTION, '^\\w+')))
#rename column to word for future analysis
lincsEvents <- lincsEvents %>% rename(word = QUOTE_TRANSCRIPTION)
#Delete the first digit from the allegation column. This removes McLane's index.
lincsEvents <- lincsEvents %>% mutate(word = (str_replace(word, '^\\d+', "")))
#Convert lincsID to numeric column
lincsEvents[, c(3)] <- sapply(lincsEvents[, c(3)], as.numeric)
lincsIDS <- lincsEvents %>% select(lincsID, UUID, word)
verbDetails <- verbDetails %>% rename(UUID = doc_id)
verbDetails <- verbDetails %>% inner_join(lincsIDS)
library('scales')
verbDetails <-  verbDetails %>%
mutate(tag = case_when(tag == "VBD" ~ "Past Tense", tag == "VB" ~ "Inf.", tag == "VBN" ~ "Past Ptp", tag == "VBG" ~ "Gerund/Pres Ptp", tag == "MD" ~ "Aux", tag == "VBP" ~ "Pres.non3P", tag == "VBZ" ~ "Pres.3rdPer"))
verbDetails %>% count(tag, sort = TRUE) %>% mutate(tag = reorder(tag,n)) %>% ggplot(aes(tag, n)) + geom_col(fill="#ffd700") + scale_x_discrete(labels = label_wrap(4)) + xlab(NULL) + coord_flip() + theme_minimal()
verbDetails %>% count(lemma, sort = TRUE) %>% filter(n > 40) %>% mutate(lemma = reorder(lemma,n)) %>% ggplot(aes(lemma, n)) + geom_col(fill="#ffd700") + xlab(NULL) + coord_flip() + theme_minimal()
links <- verbDetails %>% filter(lemma == 'see')
links
blogdown:::serve_site()
library(tidyverse)
library(tidytext)
#setwd("C:/Users/Jack/Documents/GitHub/bi-grams")
#load(".Rdata")
#un-nest lincsEvents
lincsEventsTokens <- lincsEvents %>% unnest_tokens(word, word)
#Filter for stop words
lincsStopped <- lincsEventsTokens %>% anti_join(stop_words)
#filter for digits
lincsNoDigit<- lincsEventsTokens %>% filter(grepl('^\\D', word))
#Create a list of Personal names from the original data
lincsPeople <- filter(lincsOCR, TYPE == "PERSON")
#Select desired columns from original text
#lincsPeople <- lincsPeople %>% select(QUOTE_TRANSCRIPTION, TYPE, UUID)
#Unnest Tokens
lincsPeople <- lincsPeople %>% unnest_tokens(word, QUOTE_TRANSCRIPTION)
#Removing Personal name using anti_join()
lincsPeopleStopped <- lincsEventsTokens %>% anti_join(lincsPeople, by = 'word')
#Creatng a set of place names from the text and removing them
lincsPlaces <- filter(lincsOCR, TYPE == "PLACE")
#Select desired columns from original text
#lincsPlaces <- lincsPlaces %>% select(QUOTE_TRANSCRIPTION, TYPE, UUID)
#Unnest Tokens
lincsPlaces <- lincsPlaces %>% unnest_tokens(word, QUOTE_TRANSCRIPTION)
#Removing Personal name using anti_join()
lincsPlacesStopped <- lincsEventsTokens %>% anti_join(lincsPlaces, by = 'word')
#Join tables to a single dataframe with no digits, places, or people
lincsTotalTidy <- lincsEventsTokens %>% anti_join(stop_words)
lincsTotalTidy <- lincsTotalTidy %>% filter(grepl('^\\D', word))
lincsTotalTidy <- lincsTotalTidy %>% anti_join(lincsPeople, by = 'word')
lincsTotalTidy <- lincsTotalTidy %>% anti_join(lincsPlaces, by = 'word')
lincs_words <- lincsEventsTokens %>% count(Lincolnshire_ID, word, sort = TRUE)
View(lincsEventsTokens)
blogdown:::preview_site()
lincs_words <- lincsEventsTokens %>% count(lincsID, word, sort = TRUE)
total_words <- lincs_words %>% group_by(lincsID) %>%  summarize(total = sum(n))
lincs_words <- left_join(lincs_words, total_words)
lincs_words
freq_by_rank <- lincs_words %>%  group_by(lincsID) %>% mutate(rank = row_number(), 'term frequency' = n/total) %>%  ungroup()
freq_by_rank
lincs_tf_idf <- lincs_words %>% bind_tf_idf(word, lincsID, n)
lincs_tf_idf
lincs_tf_idf %>%
select(-total) %>%
arrange(desc(tf_idf))
#Removes entries containing personal names, place names, digits, and common english words.
stop_tf_idf <- lincs_tf_idf %>% anti_join(lincs_stop_people)
stop_tf_idf <- stop_tf_idf %>% anti_join(lincs_stop_places)
stop_tf_idf <- stop_tf_idf %>% filter(grepl('^\\D', word))
stop_tf_idf <- stop_tf_idf %>% anti_join(stop_words)
stop_tf_idf %>%
select(-total) %>%
arrange(desc(tf_idf))
#Join tables to a single dataframe with no digits, places, or people
lincsTotalTidy <- lincsEventsTokens %>% anti_join(stop_words)
lincsTotalTidy <- lincsTotalTidy %>% filter(grepl('^\\D', word))
lincsTotalTidy <- lincsTotalTidy %>% anti_join(lincsPeople, by = 'word')
lincsTotalTidy <- lincsTotalTidy %>% anti_join(lincsPlaces, by = 'word')
lincsYearWords <- full_join(lincsTotalTidy, fullDated, by = 'UUID')
lincsYearWords <- lincsYearWords %>% select(UUID, lincsID, word, day, month, year, date)
lincsYearWords <- lincsYearWords %>% count(year, word, sort = TRUE)
lincsTotalYearWords <- lincsYearWords %>% group_by(year) %>%  summarize(total = sum(n))
lincsTotalYearWords <- left_join(lincsYearWords, lincsTotalYearWords)
#Remove all rows without a year
lincsTotalYearWords <- lincsTotalYearWords %>%  na.omit()
lincsTotalYearWords
yearFreqRank <- lincsTotalYearWords %>%  group_by(year) %>% mutate(rank = row_number(), 'term frequency' = n/total) %>%  ungroup()
yearFreqRank
lincsYearTFIDF <- lincsTotalYearWords %>% bind_tf_idf(word, year, n)
lincsYearTFIDF %>%
filter(n>8) %>%
arrange(desc(tf_idf))
yearFreqRank <- lincsTotalYearWords %>%  group_by(year) %>% mutate(rank = row_number(), 'term frequency' = n/total) %>%  ungroup()
yearFreqRank
lincsYearTFIDF <- lincsTotalYearWords %>% bind_tf_idf(word, year, n)
lincsYearTFIDF %>%
filter(n>8) %>%
arrange(desc(tf_idf))
#Number of occurrences of each distinct word once names and sums of money are removed
wordCounts <- lincsTotalTidy %>% count(word, sort = TRUE)
wordCounts <- wordCounts %>% rename('allegation' = 'word')
lincsYearTFIDF <- lincsYearTFIDF %>% rename('allegation' = word)
TFIDFCounts <- left_join(lincsYearTFIDF, wordCounts)
lincs_words <- Lincolnshire_Tokens %>% count(Lincolnshire_ID, word, sort = TRUE)
total_words <- lincs_words %>% group_by(Lincolnshire_ID) %>%  summarize(total = sum(n))
lincs_words <- left_join(lincs_words, total_words)
lincs_words
freq_by_rank <- lincs_words %>%  group_by(Lincolnshire_ID) %>% mutate(rank = row_number(), 'term frequency' = n/total) %>%  ungroup()
freq_by_rank
lincs_tf_idf <- lincs_words %>% bind_tf_idf(word, Lincolnshire_ID, n)
lincs_tf_idf
lincs_tf_idf %>%
select(-total) %>%
filter(n>10) %>%
arrange(desc(tf_idf))
#Removes entries containing personal names, place names, digits, and common english words.
stop_tf_idf <- lincs_tf_idf %>% anti_join(lincs_stop_people)
stop_tf_idf <- stop_tf_idf %>% anti_join(lincs_stop_places)
stop_tf_idf <- stop_tf_idf %>% filter(grepl('^\\D', word))
stop_tf_idf <- stop_tf_idf %>% anti_join(stop_words)
stop_tf_idf %>%
filter(n>3) %>%
arrange(desc(tf))
View(lincsTotalTidy)
lincsYearWords <- full_join(lincsTotalTidy, fullDated, by = 'UUID')
lincsYearWords <- lincsYearWords %>% select(UUID, lincsID, word, day, month, year, date)
lincsYearWords <- lincsYearWords %>% count(year, word, sort = TRUE)
lincsTotalYearWords <- lincsYearWords %>% group_by(year) %>%  summarize(total = sum(n))
lincsTotalYearWords <- left_join(lincsYearWords, lincsTotalYearWords)
#Remove all rows without a year
lincsTotalYearWords <- lincsTotalYearWords %>%  na.omit()
lincsTotalYearWords
lincsTotalYearWords
source('~/GitHub/blog/Data processing.R', echo=TRUE)
#setwd("C:/Users/Jack/Documents/GitHub/bi-grams")
#load(".Rdata")
getwd()
library(tidyverse)
library(tidytext)
setwd("C:/Users/Jack/Documents/GitHub/blog")
load(".Rdata")
blogdown:::preview_site()
blogdown:::serve_site()
blogdown:::serve_site()
blogdown:::new_post_addin()
blogdown:::serve_site()
library(blogdown)
blogdown:::serve_site()
blogdown:::build_site()
blogdown:::stop_server()
blogdown:::serve_site()
View(inquestGeo)
View(places)
View(burghVol3parsed)
source("~/GitHub/blog/Data processing.R", echo=TRUE)
blogdown:::new_post_addin()
install.packages(c("askpass", "blob", "blogdown", "bookdown", "broom", "bslib", "cachem", "chatgpt", "classInt", "cli", "commonmark", "cpp11", "credentials", "curl", "dbplyr", "digest", "dplyr", "DT", "dtplyr", "evaluate", "fansi", "fontawesome", "fs", "gargle", "geometries", "gert", "ggplot2", "googledrive", "googlesheets4", "gptstudio", "gtable", "haven", "hms", "htmltools", "htmlwidgets", "httpuv", "httr", "httr2", "hunspell", "jsonlite", "knitr", "labeling", "later", "leafem", "leaflet", "leaflet.providers", "lifecycle", "lubridate", "markdown", "modelr", "openssl", "packrat", "pdftools", "pillar", "plyr", "prettyunits", "processx", "promises", "ps", "purrr", "qpdf", "ragg", "raster", "Rcpp", "readxl", "rematch", "remotes", "renv", "reticulate", "RgoogleMaps", "rlang", "rmarkdown", "rnaturalearth", "rprojroot", "rsconnect", "rstudioapi", "sass", "servr", "sf", "sfheaders", "shiny", "SnowballC", "sp", "stars", "svglite", "sys", "systemfonts", "terra", "textshaping", "tibble", "tinytex", "tmap", "tzdb", "units", "usethis", "utf8", "uuid", "vctrs", "viridis", "viridisLite", "vroom", "withr", "wk", "xfun", "XML", "xml2", "zip"))
install.packages(c("askpass", "blob", "blogdown", "bookdown", "broom", "bslib", "cachem", "chatgpt", "classInt", "cli", "commonmark", "cpp11", "credentials", "curl", "dbplyr", "digest", "dplyr", "DT", "dtplyr", "evaluate", "fansi", "fontawesome", "fs", "gargle", "geometries", "gert", "ggplot2", "googledrive", "googlesheets4", "gptstudio", "gtable", "haven", "hms", "htmltools", "htmlwidgets", "httpuv", "httr", "httr2", "hunspell", "jsonlite", "knitr", "labeling", "later", "leafem", "leaflet", "leaflet.providers", "lifecycle", "lubridate", "markdown", "modelr", "openssl", "packrat", "pdftools", "pillar", "plyr", "prettyunits", "processx", "promises", "ps", "purrr", "qpdf", "ragg", "raster", "Rcpp", "readxl", "rematch", "remotes", "renv", "reticulate", "RgoogleMaps", "rlang", "rmarkdown", "rnaturalearth", "rprojroot", "rsconnect", "rstudioapi", "sass", "servr", "sf", "sfheaders", "shiny", "SnowballC", "sp", "stars", "svglite", "sys", "systemfonts", "terra", "textshaping", "tibble", "tinytex", "tmap", "tzdb", "units", "usethis", "utf8", "uuid", "vctrs", "viridis", "viridisLite", "vroom", "withr", "wk", "xfun", "XML", "xml2", "zip"))
